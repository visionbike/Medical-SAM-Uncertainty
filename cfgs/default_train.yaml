ExpConfig:
  # train or test
  mode: train
  # whether to use gpu
  use_gpu: True
  # which gpu device id to use
  gpu_device: 0
  # multiple gpu ids to use, with null is None
  distributed: null
  # checkpoint path for pretrained model, with null is None
  pretrain: sam_vit_b_01ec64.pth
  # checkpoint path for trained model, with null is None
  ckpt: ./logs/train_isic_sam_adapt_128_2025-01-08-15-24-39/ckpt/checkpoint_best.pth
  # number of training epochs
  epochs: 5
  # validation frequency
  val_freq: 1
  # visualization frequency
  vis_freq: 8
DataConfig:
  # the dataset name: isic, refuge, ddti, stare, idrid
  dataset: isic
  # dataset path stored in ./data
  path: ./data/ISIC2016
  # image size
  image_size: 128
  # output size
  output_size: 128
  # batch size
  batch_size: 8
  # number of workers
  workers: 4
  # whether to shuffle dataset
  shuffle: True
  # the number of mask outputs for multi-class segmentation:
  # 1: for [ISIC, DDTI, STARE] datasets
  # 2: for [REFUGE] dataset
  # 5: IDRiD: for [IDRiD] dataset
  multimask_output: 1
NetworkConfig:
  # network type: "sam", "mobile_sam_v2"
  net: sam
  # encoder type: "default", "vit_t", "vit_b", "vit_l", "vit_h"
  encoder: default
  # block type: "default", "adapt", "lora", "adalora"
  block: adapt
  # middle dim of adapter or the rank of lora matrix
  # default: null
  mid_dim: null
OptimConfig:
  # optimizer name: null, "adam"
  optimizer: adam
  # lr scheduler name: null, "step"
  lr_scheduler: step
  # learning rate
  lr: 0.0001
LossConfig:
  # criterion name: "bce_w_logit", "dice"
  loss: bce_w_logit
MetricConfig:
  # metric names: "dice", "dice_coeff "mae", "entropy", "iou", "corr_coeff"
  metrics: [dice_coeff, iou, mae, entropy, corr_coeff]
  thresholds: [0.1, 0.3, 0.5, 0.7, 0.9]